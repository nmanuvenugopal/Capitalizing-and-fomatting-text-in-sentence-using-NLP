{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viQFIBnuPy6e",
        "outputId": "99ea8a6c-7a81-4520-d5e4-1b7de538bd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (4.64.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->stanfordnlp) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanfordnlp) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stanfordnlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o42-H8IRQCCr",
        "outputId": "1428d0e2-f998-4643-cb95-081d942a08a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bleu in /usr/local/lib/python3.7/dist-packages (0.3)\n",
            "Requirement already satisfied: efficiency in /usr/local/lib/python3.7/dist-packages (from bleu) (1.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from efficiency->bleu) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.21.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.9.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (3.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (4.64.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (3.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (1.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->efficiency->bleu) (8.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->efficiency->bleu) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy->efficiency->bleu) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->efficiency->bleu) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->efficiency->bleu) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->efficiency->bleu) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->efficiency->bleu) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy->efficiency->bleu) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy->efficiency->bleu) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->efficiency->bleu) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "import stanfordnlp\n",
        "from bleu import list_bleu"
      ],
      "metadata": {
        "id": "T_R0_BHId8wf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr-YlXrZuU4E",
        "outputId": "46f4f3bf-6228-4bb6-a7e9-74538b1ab7ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stanfordnlp.download('en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhYQfeHrugi3",
        "outputId": "8c513a8e-46f8-4c2a-c211-292e02ca3038"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "Y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 235M/235M [00:40<00:00, 5.84MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline for the standford model\n",
        "# We will be using mwt model\n",
        "\n",
        "stan_nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbkfbifgupGL",
        "outputId": "50277b9e-809a-4901-df3e-67b437124180"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text that we need to make proper with correct capitalization and punctuation.\n",
        "\n",
        "text = \"i think that john stone is a nice guy. there is a stone on the grass. i'm fat. are you welcome and smart in london? is this martin's dog?\""
      ],
      "metadata": {
        "id": "b5aT0_I-hf11"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting of paragraph into sentences by using NLTK tokenize function using punkt\n",
        "\n",
        "sentences = sent_tokenize(text, language='english')\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyg-BSwRuLy5",
        "outputId": "7f7ab584-22fa-4af3-c3a6-78a200966498"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i think that john stone is a nice guy.',\n",
              " 'there is a stone on the grass.',\n",
              " \"i'm fat.\",\n",
              " 'are you welcome and smart in london?',\n",
              " \"is this martin's dog?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Capitalizing the first word for every tokenized sentence\n",
        "\n",
        "capitalized_sentences = []\n",
        "for sentence in sentences:\n",
        "  capitalized_sentences.append(sentence.capitalize())\n",
        "\n",
        "capitalized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caufQlPcvet3",
        "outputId": "11274b71-a8dd-4b90-eb58-62df4f3a7684"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I think that john stone is a nice guy.',\n",
              " 'There is a stone on the grass.',\n",
              " \"I'm fat.\",\n",
              " 'Are you welcome and smart in london?',\n",
              " \"Is this martin's dog?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text_true = ' '.join(capitalized_sentences)\n",
        "#text_true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MFbhZPD_xCwO",
        "outputId": "60080a08-6f53-4b15-e4fd-9e6c7548f0e1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I think that john stone is a nice guy. There is a stone on the grass. I'm fat. Are you welcome and smart in london? Is this martin's dog?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# join the capitalized sentences\n",
        "text_truecase = re.sub(\" (?=[\\.,'!?:;])\", \"\", ' '.join(capitalized_sentences))\n",
        "text_truecase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zSRP2hfJwcbT",
        "outputId": "0354986c-1c80-463e-e8aa-d870cfe48ce8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I think that john stone is a nice guy. There is a stone on the grass. I'm fat. Are you welcome and smart in london? Is this martin's dog?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Capitalizing the words according to part of speech using Stanfordnlp Pipelines\n",
        "\n",
        "text_doc = stan_nlp(text_truecase)\n",
        "text_doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpAQjRKrw6Hk",
        "outputId": "eedec78a-d670-4270-9549-071546643239"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stanfordnlp.pipeline.doc.Document at 0x7ff9c25b80d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What stanford pipeline does is that split the sentences based on POS (Verb, Proper Noun, etc.)\n",
        "# https://universaldependencies.org/u/pos/\n",
        "\n",
        "for sent in text_doc.sentences:\n",
        "  for token in sent.tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md-deQqcx6fS",
        "outputId": "5d3762d5-3caa-41ea-a2bc-cb6ccf0583de"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Token index=1;words=[<Word index=1;text=I;upos=PRON;xpos=PRP;feats=Case=Nom|Number=Sing|Person=1|PronType=Prs>]>\n",
            "<Token index=2;words=[<Word index=2;text=think;upos=VERB;xpos=VBP;feats=Mood=Ind|Tense=Pres|VerbForm=Fin>]>\n",
            "<Token index=3;words=[<Word index=3;text=that;upos=SCONJ;xpos=IN;feats=_>]>\n",
            "<Token index=4;words=[<Word index=4;text=john;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=5;words=[<Word index=5;text=stone;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=6;words=[<Word index=6;text=is;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>\n",
            "<Token index=7;words=[<Word index=7;text=a;upos=DET;xpos=DT;feats=Definite=Ind|PronType=Art>]>\n",
            "<Token index=8;words=[<Word index=8;text=nice;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
            "<Token index=9;words=[<Word index=9;text=guy;upos=NOUN;xpos=NN;feats=Number=Sing>]>\n",
            "<Token index=10;words=[<Word index=10;text=.;upos=PUNCT;xpos=.;feats=_>]>\n",
            "<Token index=1;words=[<Word index=1;text=There;upos=PRON;xpos=EX;feats=_>]>\n",
            "<Token index=2;words=[<Word index=2;text=is;upos=VERB;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>\n",
            "<Token index=3;words=[<Word index=3;text=a;upos=DET;xpos=DT;feats=Definite=Ind|PronType=Art>]>\n",
            "<Token index=4;words=[<Word index=4;text=stone;upos=NOUN;xpos=NN;feats=Number=Sing>]>\n",
            "<Token index=5;words=[<Word index=5;text=on;upos=ADP;xpos=IN;feats=_>]>\n",
            "<Token index=6;words=[<Word index=6;text=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>\n",
            "<Token index=7;words=[<Word index=7;text=grass;upos=NOUN;xpos=NN;feats=Number=Sing>]>\n",
            "<Token index=8;words=[<Word index=8;text=.;upos=PUNCT;xpos=.;feats=_>]>\n",
            "<Token index=1;words=[<Word index=1;text=I;upos=PRON;xpos=PRP;feats=Case=Nom|Number=Sing|Person=1|PronType=Prs>]>\n",
            "<Token index=2;words=[<Word index=2;text='m;upos=AUX;xpos=VBP;feats=Mood=Ind|Tense=Pres|VerbForm=Fin>]>\n",
            "<Token index=3;words=[<Word index=3;text=fat;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
            "<Token index=4;words=[<Word index=4;text=.;upos=PUNCT;xpos=.;feats=_>]>\n",
            "<Token index=1;words=[<Word index=1;text=Are;upos=AUX;xpos=VBP;feats=Mood=Ind|Tense=Pres|VerbForm=Fin>]>\n",
            "<Token index=2;words=[<Word index=2;text=you;upos=PRON;xpos=PRP;feats=Case=Nom|Person=2|PronType=Prs>]>\n",
            "<Token index=3;words=[<Word index=3;text=welcome;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
            "<Token index=4;words=[<Word index=4;text=and;upos=CCONJ;xpos=CC;feats=_>]>\n",
            "<Token index=5;words=[<Word index=5;text=smart;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>\n",
            "<Token index=6;words=[<Word index=6;text=in;upos=ADP;xpos=IN;feats=_>]>\n",
            "<Token index=7;words=[<Word index=7;text=london;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=8;words=[<Word index=8;text=?;upos=PUNCT;xpos=.;feats=_>]>\n",
            "<Token index=1;words=[<Word index=1;text=Is;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>\n",
            "<Token index=2;words=[<Word index=2;text=this;upos=DET;xpos=DT;feats=Number=Sing|PronType=Dem>]>\n",
            "<Token index=3;words=[<Word index=3;text=martin;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=4;words=[<Word index=4;text='s;upos=PART;xpos=POS;feats=_>]>\n",
            "<Token index=5;words=[<Word index=5;text=dog;upos=NOUN;xpos=NN;feats=Number=Sing>]>\n",
            "<Token index=6;words=[<Word index=6;text=?;upos=PUNCT;xpos=.;feats=_>]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to capitalize the all the Proper Nouns.\n",
        "# NNS ?\n",
        "\n",
        "final_words = []\n",
        "for sent in text_doc.sentences:\n",
        "  for word in sent.words:\n",
        "    if word.upos in ['PROPN','NNS']:\n",
        "      final_words.append(word.text.capitalize())\n",
        "    else:\n",
        "      final_words.append(word.text)\n",
        "\n",
        "final_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDx6aIuByUqd",
        "outputId": "e0533968-8f0f-4aa8-a115-277215728552"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'think',\n",
              " 'that',\n",
              " 'John',\n",
              " 'Stone',\n",
              " 'is',\n",
              " 'a',\n",
              " 'nice',\n",
              " 'guy',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'stone',\n",
              " 'on',\n",
              " 'the',\n",
              " 'grass',\n",
              " '.',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'fat',\n",
              " '.',\n",
              " 'Are',\n",
              " 'you',\n",
              " 'welcome',\n",
              " 'and',\n",
              " 'smart',\n",
              " 'in',\n",
              " 'London',\n",
              " '?',\n",
              " 'Is',\n",
              " 'this',\n",
              " 'Martin',\n",
              " \"'s\",\n",
              " 'dog',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now new can join the word to get the final string\n",
        "\n",
        "output_sentence = ' '.join(final_words)\n",
        "output_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sU_OR2Ru2CKq",
        "outputId": "1db26858-ab30-4a58-e3b1-38b0f9328c26"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I think that John Stone is a nice guy . There is a stone on the grass . I 'm fat . Are you welcome and smart in London ? Is this Martin 's dog ?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBPAAZhP3Vhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}